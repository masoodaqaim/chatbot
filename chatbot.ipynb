{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning how to build a chatbox. Following Tech with Time (https://www.youtube.com/watch?v=wypVcNIH6D4&list=PLzMcBGfZo4-ndH9FoC4YWHGXG5RZekt-Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intent file is formated as a JSON file. Here is the format:\n",
    "{\"intents\": [\n",
    "        {\"tag\": \"greating\",\n",
    "        \"patterns\": [\"Hi\"],\n",
    "        \"response\" [\"Hello\"],\n",
    "        \"context_set\": \"\"\n",
    "        }\n",
    "        \n",
    "The basic structor is as follows:\n",
    "    - person types a message\n",
    "    - the chatbot, through deeplearning, tags the message\n",
    "    - the chatbot then returns with one of the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading our json file\n",
    "with open ('intents.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "#print(data)  # just to make sure we correctly loaded/read our file\n",
    "#print(data['intents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save each patter/grouping into a variable \n",
    "words = []\n",
    "labels = []\n",
    "docs = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patters']:\n",
    "        # stemming will break each word in our pattern and break it down into the root word\n",
    "        # ex. \"Is anyone there?\": the root is \"there\" ignoreing the other words and ?\n",
    "        # we need to tokkenize (getting all the words)\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend.wrds\n",
    "        docs.append(pattern)\n",
    "        \n",
    "    if intent['tag'] not in lables:\n",
    "            labels.append(intent['tag'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
